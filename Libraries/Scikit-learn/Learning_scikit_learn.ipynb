{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1706e107-4a20-45c1-a269-d4dffb9c8f64",
   "metadata": {},
   "source": [
    "# Learning Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24891a-594c-4f8a-8584-d2fb618a740a",
   "metadata": {},
   "source": [
    "Scikit-learn is a powerful machine learning library in Python that provides tools for data preprocessing, model training, evaluation, and deployment. Here's an overview of its core modules and their functionalities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b88ca-6945-4651-99c2-c0f96fcc4a30",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "### Module: sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f806406-9253-4474-8f20-eb0c6be2578e",
   "metadata": {},
   "source": [
    "- **Purpose**: Prepare data for machine learning algorithms by standardizing, normalizing, or encoding it.\n",
    "- **Key Tools**:\n",
    ">- **StandardScaler**: Standardizes features by removing the mean and scaling to unit variance.\n",
    ">- **MinMaxScaler**: Scales features to a specific range, usually [0, 1].\n",
    ">- **OneHotEncoder**: Converts categorical variables into a one-hot numeric array.\n",
    ">- **LabelEncoder**: Converts labels into integers.\n",
    ">- **PolynomialFeatures**: Generates polynomial and interaction features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775dad3-7d6f-42f5-bca2-7f97938cc3cf",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "### Module: sklearn.feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d76df5-df26-45b0-b630-c0a5623401b3",
   "metadata": {},
   "source": [
    "- **Purpose**: Identify and select the most relevant features in your dataset.\n",
    "- **Key Tools**:\n",
    ">- **SelectKBest**: Selects the top k features based on statistical tests.\n",
    ">- **RFE (Recursive Feature Elimination)**: Recursively removes the least important features.\n",
    ">- **VarianceThreshold**: Removes features with low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc27a74-9a66-458e-b883-73e9ae9e249f",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction\n",
    "### Module: sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c5819-7504-48ef-8628-5148baeed7cd",
   "metadata": {},
   "source": [
    "- **Purpose**: Reduce the number of features while retaining important information.\n",
    "- **Key Tools**:\n",
    ">- **PCA (Principal Component Analysis)**: Reduces dimensionality by finding the principal components.\n",
    ">- **TruncatedSVD**: Performs dimensionality reduction on sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a9628-18be-4a9b-aee3-56d07a4c495e",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Validation\n",
    "### Module: sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad7668-e1a0-495b-bf00-94a2550d60a9",
   "metadata": {},
   "source": [
    "- **Purpose**: Split data into training and testing sets, and perform cross-validation.\n",
    "- **Key Tools**:\n",
    ">- **train_test_split**: Splits data into training and test sets.\n",
    ">- **GridSearchCV**: Performs hyperparameter tuning using exhaustive search.\n",
    ">- **RandomizedSearchCV**: Performs hyperparameter tuning using randomized search.\n",
    ">- **KFold**: Splits data for k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a97d5-4c19-4ac8-8943-8e18ec5eff72",
   "metadata": {},
   "source": [
    "## 5. Classification\n",
    "### Module: sklearn.linear_model, sklearn.svm, sklearn.tree, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f44176-9455-4bdc-9db9-df866b993780",
   "metadata": {},
   "source": [
    "- **Purpose**: Algorithms for classifying data into categories.\n",
    "- **Key Models**:\n",
    ">- **Logistic Regression**: From sklearn.linear_model.\n",
    ">- **Support Vector Machines (SVM)**: From sklearn.svm.\n",
    ">- **Decision Trees**: From sklearn.tree.\n",
    ">- **Naive Bayes**: From sklearn.naive_bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc8111-aedc-48cf-9259-6cdb1c9f1bde",
   "metadata": {},
   "source": [
    "## 6. Regression\n",
    "### Module: sklearn.linear_model, sklearn.svm, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c41ff5-fc3a-439b-9019-676695c7d3a4",
   "metadata": {},
   "source": [
    "- **Purpose**: Algorithms for predicting continuous values.\n",
    "- **Key Models**:\n",
    ">- **Linear Regression**: From sklearn.linear_model.\n",
    ">- **Ridge/Lasso Regression**: Regularized regression models.\n",
    ">- **SVR (Support Vector Regression)**: From sklearn.svm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc3bee-52d5-4d33-9c63-5f6d217d24f3",
   "metadata": {},
   "source": [
    "## 7. Clustering\n",
    "### Module: sklearn.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699c0a7-5601-45b0-9ba0-6ba55b288cb7",
   "metadata": {},
   "source": [
    "- **Purpose**: Group similar data points together without predefined labels.\n",
    "- **Key Models**:\n",
    ">- **KMeans**: Clusters data into k clusters.\n",
    ">- **DBSCAN**: Groups data based on density.\n",
    ">- **AgglomerativeClustering**: Hierarchical clustering method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b351bc29-2629-43e3-9fa6-63dd1240a2a2",
   "metadata": {},
   "source": [
    "## 8. Ensemble Methods\n",
    "### Module: sklearn.ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12dff6b-159a-40a9-9d54-4091b31a7f1b",
   "metadata": {},
   "source": [
    "- **Purpose**: Combine multiple models for better performance.\n",
    "- **Key Models**:\n",
    ">- **RandomForest**: Ensemble of decision trees.\n",
    ">- **GradientBoosting**: Builds models sequentially to correct errors.\n",
    ">- **VotingClassifier**: Combines predictions from multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d866cf-3d60-456c-b640-9c43fbda80ab",
   "metadata": {},
   "source": [
    "## 9. Metrics\n",
    "### Module: sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23654c45-0695-4a5b-87af-7ad2d4f66c2d",
   "metadata": {},
   "source": [
    "- **Purpose**: Evaluate model performance.\n",
    "- **Key Tools**:\n",
    ">- **accuracy_score**, **f1_score**, **precision_score**, **recall_score** for classification.\n",
    ">- **mean_squared_error**, **r2_score** for regression.\n",
    ">- **confusion_matrix** and **roc_auc_score** for detailed performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1c9fb-8364-477b-a7c3-369a93bb977b",
   "metadata": {},
   "source": [
    "## 10. Pipeline\n",
    "### Module: sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb3441-1361-444f-a2fc-2dc7e6aa6d4d",
   "metadata": {},
   "source": [
    "- **Purpose**: Combine preprocessing steps and modeling into a single workflow.\n",
    "- **Key Tools**:\n",
    ">- **Pipeline**: Chains multiple steps together.\n",
    ">- **FeatureUnion**: Combines multiple feature extraction methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74daaf1-146b-44fb-8835-09ae32f25381",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression is one of the most fundamental and widely used machine learning algorithms for predicting a continuous target variable. It models the relationship between independent variables (features) and a dependent variable (target) by fitting a linear equation to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5dc4b-2d36-4719-a0aa-a9e36ef58c13",
   "metadata": {},
   "source": [
    "### Equation:\n",
    "$y=w_1x_1+w_2x_2+...+w_nx_n+b$,\n",
    "where:\n",
    "- $y$: Predicted value.\n",
    "- $x_1, x_2, ...,x_n$: Features.\n",
    "- $w_1, w_2, ..., w_n$: Weights (coefficients) to be learned.\n",
    "- $b$: Bias term (intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2cb58-63fd-425a-b78e-d5f9b155f6dd",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "Minimize the error between predicted and actual values. This is achieved by minimizing a loss function, commonly the Mean Squared Error (MSE):\n",
    "$$ MSE=\\frac{1}{n}\\sum\\limits _{i=1}^{n}(y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a778611-ba0a-4e18-b111-35de8598b5ce",
   "metadata": {},
   "source": [
    "## Implementation with Scikit-Learn\n",
    "Here’s how to implement linear regression in Python using scikit-learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8775641-e4bf-495c-9840-06a515d6f5c1",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72581a34-f4de-4203-8935-064e76558c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15cd65-b4fd-4331-9eab-993cbcc4f47a",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Data\n",
    "Let’s assume you have a dataset data.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b044159-aa38-4ea8-ad0d-ceda3e71dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X = df[['feature1', 'feature2']]  # Replace with your features\n",
    "y = df['target']  # Replace with your target variable\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed7d80-19b7-4aee-8591-73ce845cb0fc",
   "metadata": {},
   "source": [
    "### Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf58fb8-927d-42de-928c-298aa54317f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Coefficients and intercept\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef2c35-bec9-46e0-ac81-0f1acb551a02",
   "metadata": {},
   "source": [
    "### Step 4: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a0d65-586d-48c2-9f0c-b3fa040e0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115e6a1-21b7-4b3e-9bb6-eaf4fb1abb87",
   "metadata": {},
   "source": [
    "## Visualize the Results\n",
    "For a single feature, you can visualize the regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65070a2-6e38-4d4e-b480-2187bdde5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.scatter(X_test['feature1'], y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test['feature1'], y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb632e-3901-4a34-9850-539ff6e544a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d17f15-5157-4861-ba53-154ccdc64428",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13a5c0-9ce9-4852-8ed4-4408bca6afac",
   "metadata": {},
   "source": [
    "Preprocessing in machine learning involves transforming raw data into a clean, normalized, and optimized form to make it suitable for training machine learning models. In scikit-learn, the **sklearn.preprocessing** module provides a variety of tools for preprocessing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecaf32-5d69-433f-bbff-848321246341",
   "metadata": {},
   "source": [
    "## Why Preprocess?\n",
    "**1. Consistency**: Ensure all features are on the same scale.\n",
    "\n",
    "**2. Handling Missing Data**: Replace missing values appropriately.\n",
    "\n",
    "**3. Encoding Categorical Data**: Convert categorical variables to numeric \n",
    "format.\n",
    "\n",
    "**4. Improving Model Performance**: Optimize data for better learning.\n",
    "\n",
    "**5. Reducing Noise**: Clean data to focus on relevant patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e666818-caa7-43ee-a73c-2abfcf5a402e",
   "metadata": {},
   "source": [
    "## Key Preprocessing Techniques\n",
    "Here’s a breakdown of the most common preprocessing techniques in scikit-learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf65e4-cb16-4a5d-8554-910d454681fa",
   "metadata": {},
   "source": [
    "### 1. Scaling Features\n",
    "Scaling ensures all features contribute equally to the model by bringing them to the same scale. Without scaling, features with larger ranges dominate the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de167c9-3721-4ad5-8660-0007b8a22c28",
   "metadata": {},
   "source": [
    "#### StandardScaler\n",
    "- Standardizes features by removing the mean and scaling to unit variance. After scaling, the data will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "- Formula: $Z=\\frac{x-mean}{std}$\n",
    "\n",
    "- **When to use:**\n",
    "When your data follows a **Gaussian (normal) distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36634131-3a61-4d74-964b-3690e2ddaefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.06904497]\n",
      " [ 0.         -0.26726124]\n",
      " [ 1.22474487  1.33630621]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "X = [[1, 10], [2, 15], [3, 25]]\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc624a4-ccb8-4330-a773-a0985608731e",
   "metadata": {},
   "source": [
    "#### MinMaxScaler\n",
    "- Scales features to a specific range, typically [0, 1].\n",
    "\n",
    "- Formula: $x^\\prime = \\frac{x-min}{max-min}$\n",
    "\n",
    "- **When to use:**\n",
    "When you want all features to be within a specific range, useful for algorithms like **neural networks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a60451ea-99c2-42b7-a874-f01833267b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.5        0.33333333]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_min_max_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "print(X_min_max_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d37f0-e732-4591-89a9-3108765c2e73",
   "metadata": {},
   "source": [
    "### 2. Encoding Categorical Variables\n",
    "Many machine learning algorithms require categorical variables to be converted into numeric representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d640e-d90e-4a7e-b8a0-551e03205494",
   "metadata": {},
   "source": [
    "#### OneHotEncoder\n",
    "- Converts categorical variables into a one-hot (binary) format. Each category gets its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be111ee-77a8-4ede-8d81-80bf3bca8f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example categorical data\n",
    "data = [['red'], ['blue'], ['green']]\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded = encoder.fit_transform(data).toarray()\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7324d8-0037-4cbd-aa15-4261b3f79584",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "  \n",
    "Each category (e.g., red, blue, green) is converted into a binary column.\n",
    "\n",
    "red=[1,0,0],blue=[0,1,0],       green=[0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81abdd2-d926-4911-8a49-cce2893f41f7",
   "metadata": {},
   "source": [
    "#### LabelEncoder\n",
    "- Converts categories into numeric labels. This is typically used for the **target variable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f89c41-25e3-4ba4-bb30-a5936dd0e314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = ['cat', 'dog', 'mouse']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "print(encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3ec93-c448-46c5-ba8f-9fb7dcf8884b",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "\n",
    "\n",
    "cat=0,dog=1,mouse=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a3d34-5e86-48f2-8c90-4a091b3aee2f",
   "metadata": {},
   "source": [
    "### 3. Imputation of Missing Values\n",
    "Handling missing data is critical because most machine learning models cannot handle NaN values.\n",
    "\n",
    "#### SimpleImputer\n",
    "- Replaces missing values with a specified strategy (mean, median, or most frequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92ef302-d1eb-499c-ae12-d103bb3cbba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [4. 3.]\n",
      " [7. 6.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "data2 = [[1, 2], [np.nan, 3], [7, 6]]\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "filled_data = imputer.fit_transform(data2)\n",
    "\n",
    "print(filled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7202e8a-e687-4e7a-95cc-4e3644b771bb",
   "metadata": {},
   "source": [
    "### 4. Generating Polynomial Features\n",
    "#### PolynomialFeatures\n",
    "- Creates new features that are combinations of existing ones.\n",
    "- Useful for polynomial regression or capturing non-linear relationships.\n",
    "- **When to use**:\n",
    "When you suspect non-linear relationships between features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1379d2ca-8185-4a4c-a3ca-2d5a97bb2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4. 6. 9.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Example data\n",
    "Y = [[2, 3]]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "Y_poly = poly.fit_transform(Y)\n",
    "\n",
    "print(Y_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc92f81-41b0-4986-9624-b3467ffb5b31",
   "metadata": {},
   "source": [
    "Original features: $[x_1, x_2]$ \n",
    "Generated features: $[1, x_1, x_2, x_1^2, x_1x_2, x_2^2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca91df-9af0-4b33-8bde-4f13ba733eee",
   "metadata": {},
   "source": [
    "### 5. Binarization\n",
    "#### Binarizer\n",
    "- Converts numeric values to binary (0 or 1) based on a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b02822-3842-401b-add2-93931fde82ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "data3 = [[1.5], [2.0], [3.5]]\n",
    "binarizer = Binarizer(threshold=2.0)\n",
    "binary_data = binarizer.fit_transform(data3)\n",
    "\n",
    "print(binary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33429100-49e0-4ace-9ccc-957db4e6d0e1",
   "metadata": {},
   "source": [
    "### 6. Normalization\n",
    "#### Normalizer\n",
    "- Normalizes feature vectors to have a unit norm.\n",
    "- Useful for text classification or clustering with sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cdb72b2-ac7f-47fe-af49-61f1c5405aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26726124 0.53452248 0.80178373]\n",
      " [0.45584231 0.56980288 0.68376346]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data4 = [[1, 2, 3], [4, 5, 6]]\n",
    "normalizer = Normalizer()\n",
    "normalized_data = normalizer.fit_transform(data4)\n",
    "\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28a642-eb13-4de0-a6e3-a90fc5d36c9b",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "\n",
    "Each row is normalized such that the sum of squared values equals 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1da55-81e4-4c1c-b93c-7c9bedb820b1",
   "metadata": {},
   "source": [
    "## Preprocessing with Pipelines\n",
    "To streamline preprocessing, you can use pipelines in scikit-learn to combine multiple steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5acaf8b-0bf2-46a2-becc-6515548f3609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.22474487 -1.06904497  1.5         1.30930734  1.14285714]\n",
      " [ 1.          0.         -0.26726124  0.         -0.          0.07142857]\n",
      " [ 1.          1.22474487  1.33630621  1.5         1.63663418  1.78571429]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example pipeline: Scaling + Polynomial Features\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2))\n",
    "])\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be865a5-1eb2-4fff-938b-af399f4d48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X, y, degree):\n",
    "    # TODO: Create and train a model based on the given degree\n",
    "    features = PolynomialFeatures(degree=degree)\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    model = make_pipeline(features, lr)    \n",
    "    model.fit(X.reshape(-1, 1), y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5823c65-56dc-4765-b026-8bcb031fea2e",
   "metadata": {},
   "source": [
    "**model = make_pipeline(features, lr)**: \n",
    "\n",
    "**make_pipeline**: A utility from sklearn.pipeline to streamline the preprocessing and model-fitting process.\n",
    "- It ensures the PolynomialFeatures step is applied first (to transform the features) before fitting the LinearRegression model.\n",
    "- This simplifies the workflow and ensures the two steps are performed in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613c647-6c52-42d9-9cbb-e7cf037c6c08",
   "metadata": {},
   "source": [
    "## How to Choose Preprocessing Techniques?\n",
    "- **Scaling**: Always scale features when your model depends on distance or gradients (e.g., SVM, KNN).\n",
    "- **Encoding**: Use encoding for categorical data.\n",
    "- **Imputation**: Fill missing values before training.\n",
    "- **Normalization**: Use for data with varying magnitudes or when vector lengths matter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8093f3f-954c-49e4-becb-1a798e26933d",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d42d6-39ba-44a4-b297-216d00000c61",
   "metadata": {},
   "source": [
    "## 1. Mean Squared Error (MSE):\n",
    "Mean Squared Error (MSE) is a commonly used metric to evaluate regression models. It measures the average squared difference between the actual values and the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19704dde-0427-491c-86bd-9f1735a86534",
   "metadata": {},
   "source": [
    "$$ MSE=\\frac{1}{n}\\sum\\limits _{i=1}^{n}(y_i-\\hat{y_i})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4256e5-bf66-46fa-9235-36488421283b",
   "metadata": {},
   "source": [
    "1. MSE penalizes large errors more than smaller ones because the errors are squared.\n",
    "2. A lower MSE indicates a better fit of the model to the data.\n",
    "3. MSE is always non-negative and has the same units as the square of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe293aa-6a0b-4e57-87e6-0e426748ad65",
   "metadata": {},
   "source": [
    "### Using mean_squared_error from Scikit-learn\n",
    "The **scikit-learn** library provides a built-in function for MSE, which simplifies the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1182ea-cf49-495d-be30-f053f2bff968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Actual and predicted values\n",
    "y_actual = [3, -0.5, 2, 7]\n",
    "y_predicted = [2.5, 0.0, 2, 8]\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_actual, y_predicted)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e2af3-16b6-45a7-b203-bd1fe1097dff",
   "metadata": {},
   "source": [
    "#### When to Use MSE\n",
    "- **Regression Models**: Evaluate how well the model predicts continuous target variables.\n",
    "- **Model Comparison**: Compare different models; the one with the lowest MSE is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d16e0e-c692-4854-9dc4-3c35c46b7a37",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Sensitive to large errors because of squaring, which can help identify poor predictions.\n",
    "- Easy to compute and widely understood.\n",
    "### Disadvantages\n",
    "- Since MSE squares errors, it can overemphasize large deviations, making it sensitive to outliers.\n",
    "- The value is in squared units, which can be harder to interpret compared to metrics like RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e75580-ec3b-4e6e-8c3f-c3ff63ccd635",
   "metadata": {},
   "source": [
    "### Real-World Example\n",
    "Suppose you train a polynomial regression model and want to calculate its MSE on the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3a89ae-5741-418a-a20d-72630d84dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 61.96330450394991\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 5 * (X**2).flatten() + np.random.normal(0, 10, X.shape[0])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a polynomial regression model\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict and calculate MSE\n",
    "y_pred = model.predict(X_test_poly)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8de245-a268-49f8-8aa1-b864f0b99656",
   "metadata": {},
   "source": [
    "### Interpreting MSE\n",
    "- A **low MSE** indicates the model is accurately predicting the target variable.\n",
    "- A **high MSE** suggests poor predictions or overfitting/underfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af7f21-f038-4e43-b32e-59b8c83214c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b03360-ab2c-4907-83e8-c214a216e18f",
   "metadata": {},
   "source": [
    "# Lasso and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcb720-4b79-42c5-ac18-3379c6313db1",
   "metadata": {},
   "source": [
    "Lasso and Ridge are two types of regularized regression methods that help prevent overfitting by adding a penalty to the regression coefficients.\n",
    "\n",
    "Both methods are useful when you have multicollinearity (high correlation between features) or when you want to control the complexity of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc795e-3600-4930-9a40-ec525bc8362a",
   "metadata": {},
   "source": [
    "## 1. Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019966b3-98c3-4773-a3b1-7d248b412082",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Ridge regression modifies Linear Regression by adding a penalty term that is the sum of the squared values of the coefficients:\n",
    "$$                    Loss Function = \\sum(y_i-\\hat{y_i})^2 + \\lambda\\sum w_j^2                         $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a59f9-2714-48a6-a768-9e0efa3a1f88",
   "metadata": {},
   "source": [
    "- The **first term** is the standard Mean Squared Error (MSE).\n",
    "- The **second term** $\\lambda\\sum w_j^2$ is the **L2 penalty** that **shrinks** the coefficients.\n",
    "- $\\lambda$ **(alpha in Python)** controls the regularization strength:\n",
    ">- If$\\lambda = 0$ → Ridge behaves like regular linear regression.\n",
    ">- if **$\\lambda$ is large** → The model forces coefficients to be **small**, reducing variance but possibly increasing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494eca7f-4a56-44d7-bf1e-3d56118132e1",
   "metadata": {},
   "source": [
    "### When to Use Ridge?\n",
    "- When all features contribute to the output, but some may have **small effects**.\n",
    "- When you want to **reduce overfitting** but **keep all features** (shrink coefficients instead of removing them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbbde3-73e5-486c-b224-43b8a142ede3",
   "metadata": {},
   "source": [
    "## 2. Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59013cd5-1498-4e91-b915-1ea58a6a3e54",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Lasso regression also adds a penalty but uses the **absolute values** of the coefficients:\n",
    "$$ Loss Function = \\sum(y_i-\\hat{y_i})^2 + \\lambda\\sum |w_j|  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89563469-ab69-404d-ab71-6893ddc11caa",
   "metadata": {},
   "source": [
    "- The **L1 penalty** ($\\lambda\\sum |w_j|$) forces some coefficients to become exactly **zero**.\n",
    "- This means Lasso **performs feature selection** by removing some features completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6df16-4ab7-4257-94a9-b22d0b969355",
   "metadata": {},
   "source": [
    "### When to Use Lasso?\n",
    "- When you suspect **some features are irrelevant** and want to **automatically select important features**.\n",
    "- When you need **a simpler, more interpretable model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de74ea62-0968-45b2-afa1-4fbd0fdc5ee0",
   "metadata": {},
   "source": [
    "## 3. Ridge vs. Lasso: Key Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb67efa-ab4d-4052-bc2f-3aa1f8a99cd8",
   "metadata": {},
   "source": [
    "|Feature | Ridge Regression (L2) | Lasso Regression (L1)|\n",
    "|---|---|---|\n",
    "|Penalty Type|$\\sum w_j^2$| $\\sum absolute(w_j)$ |\n",
    "|Effect on Coefficients|Shrinks them towards zero|Some coefficients become exactly zero|\n",
    "|Feature Selection|No|Yes (removes irrelevant features)|\n",
    "|Best for|Keeping all features but reducing their impact|Selecting only important features|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec7762-bc9c-4d67-a16b-bacc776cdb5d",
   "metadata": {},
   "source": [
    "## 4. Implementing Ridge and Lasso in Python\n",
    "We will use *Ridge* and *Lasso* from *sklearn.linear_model*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d2aa0-ce69-4297-962c-af1ce5c9de08",
   "metadata": {},
   "source": [
    "### Example: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "357fd60d-8a23-4188-99bd-58baad5a97ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Coefficients: [1.61038427]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = 3 * np.random.rand(100, 1)\n",
    "y = 5 + 2 * X + np.random.randn(100, 1)  # y = 5 + 2X + noise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Ridge Regression\n",
    "ridge_reg = Ridge(alpha=1.0)  # lambda (regularization strength)\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = ridge_reg.predict(X_test_scaled)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Ridge Coefficients:\", ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50304fe-6e32-4708-aa2a-f7f583337733",
   "metadata": {},
   "source": [
    "### Example: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaa4110-19db-4022-a6cf-8d8576951415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients: [1.53051407]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Train Lasso Regression\n",
    "lasso_reg = Lasso(alpha=0.1)  # Smaller alpha allows some coefficients to be zero\n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lasso = lasso_reg.predict(X_test_scaled)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Lasso Coefficients:\", lasso_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6a9dd-48c0-44f8-8d98-3b8f30401551",
   "metadata": {},
   "source": [
    "## 5. Choosing Between Ridge and Lasso\n",
    "### Use Ridge when:\n",
    "- You **don't want to remove features**, just **reduce their effect**.\n",
    "- You have **many correlated features**.\n",
    "- You need a **stable model** that doesn't eliminate variables.\n",
    "\n",
    "### Use Lasso when:\n",
    "- You suspect **some features are irrelevant** and want to remove them.\n",
    "- You prefer **a simpler, more interpretable model**.\n",
    "- You need **automatic feature selection**.\n",
    "\n",
    "### Use Elastic Net when:\n",
    "- You want a balance between Ridge and Lasso → **Elastic Net** combines both penalties.\n",
    "- When Ridge keeps all features, but Lasso removes too many.\n",
    "- It's useful when there are **highly correlated** features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc153622-9309-48ac-87cb-7549546de09b",
   "metadata": {},
   "source": [
    "## 6. Elastic Net Regression\n",
    "Elastic Net is a **hybrid** of **Ridge (L2) and Lasso (L1) regression** that combines both regularization techniques.\n",
    "$$                    Loss Function = \\sum(y_i-\\hat{y_i})^2 +\\lambda_1\\sum |w_j| + \\lambda_2\\sum w_j^2                         $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a9e27-17a2-482d-9e79-ba7ec3a8ce00",
   "metadata": {},
   "source": [
    "- The **first term** is the standard Mean Squared Error (MSE).\n",
    "- The **L1 penalty** ($\\lambda_1\\sum |w_j|$) encourages **sparse models** (like Lasso).\n",
    "- The **L2 penalty** ($ \\lambda_2\\sum w_j^2 $) prevents **over-shrinking** the coefficients (like Ridge).\n",
    ">- If $\\lambda_1 = 0$ → Elastic Net behaves like **Ridge**.\n",
    ">- If $\\lambda_2 = 0$ → Elastic Net behaves like **Lasso**.\n",
    ">- **If both $\\lambda_1 $ and $\\lambda_2 $ are nonzero**, Elastic Net balances Ridge and Lasso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064194a6-6f92-4bbf-9ccc-c56f1324c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Coefficients: [1.5052515]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = 3 * np.random.rand(100, 1)\n",
    "y = 5 + 2 * X + np.random.randn(100, 1)  # y = 5 + 2X + noise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Elastic Net Regression\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # 50% L1, 50% L2\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = elastic_net.predict(X_test_scaled)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Elastic Net Coefficients:\", elastic_net.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b265ab-757a-446a-9e20-633f7b726551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376306c6-07d7-4dc1-89ab-c9369a19ca7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
